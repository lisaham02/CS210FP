# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bel6wF45_NAYZW0hx9eygfDhC5PF-_oh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""**Step 1:**
Data Collection: Use a kaggle dataset, import that as a csv using pandas.
"""

# Step 1
df= pd.read_csv("/Healthcare-Diabetes.csv")

df.head(10)

# Assuming your dataframe is named df
num_entries = len(df)
print(f'The number of entries in the dataframe is: {num_entries}')

!apt-get install -y mysql-server
!pip install mysql-connector-python

!service mysql start

!mysql -e "ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'njusalisa'; FLUSH PRIVILEGES;"

"""**Step 2:** Data Storage: Transform and store the kaggle dataset efficiently to be used for a variety of data structures. This will be done by using relational databases (MySQL)."""

# Step 2
import mysql.connector

connection = mysql.connector.connect(
    host='localhost',
    user='root',
    password='njusalisa'
)
cursor = connection.cursor()

# Creating heathcare_db
cursor.execute("CREATE DATABASE IF NOT EXISTS healthcare_db")

# Creating user1 & user2 privileges
cursor.execute("CREATE USER 'user1'@'localhost' IDENTIFIED BY 'njusalisa';")
cursor.execute("CREATE USER 'user2'@'localhost' IDENTIFIED BY 'password2';")

cursor.execute("GRANT ALL PRIVILEGES ON healthcare_db.* TO 'user1'@'localhost';")
cursor.execute("GRANT ALL PRIVILEGES ON healthcare_db.* TO 'user2'@'localhost';")

# Flush privileges to apply changes
cursor.execute("FLUSH PRIVILEGES;")

# Commit and close connection
connection.commit()
cursor.close()
connection.close()

import mysql.connector

# Connect to MySQL with the new user
conn = mysql.connector.connect(
    host="localhost",
    user="user1",
    passwd="njusalisa",
    database="healthcare_db"
)
cursor = conn.cursor()

# Using healtcare_db
cursor.execute("USE healthcare_db")

# Creating Patients table
cursor.execute("""
CREATE TABLE IF NOT EXISTS patients (
    patient_id INT AUTO_INCREMENT PRIMARY KEY,
    age INT,
    outcome INT
)
""")

# Creating Health Metrics (Relational) Table
cursor.execute("""
CREATE TABLE IF NOT EXISTS health_metrics (
    metric_id INT AUTO_INCREMENT PRIMARY KEY,
    patient_id INT,
    pregnancies INT,
    glucose INT,
    blood_pressure INT,
    skin_thickness INT,
    insulin INT,
    bmi FLOAT,
    diab_pedigree_func FLOAT,
    FOREIGN KEY (patient_id) REFERENCES patients(patient_id)
)
""")

# Commit and close connection
conn.commit()
cursor.close()
conn.close()

# This part inserts data from a CSV file into two relational tables: 'patients' and 'health_metrics'.
# The healthcare dataset was reformatted to align w/ the specific schema of the relational database.

import pandas as pd
import mysql.connector

# Connect to MySQL
conn = mysql.connector.connect(
    host="localhost",
    user="user1",
    passwd="njusalisa",
    database="healthcare_db"
)
cursor = conn.cursor()

# Read the CSV file
df= pd.read_csv("/Healthcare-Diabetes.csv")

# Inserting age & outcome into the patients table
for index, row in df.iterrows():
    cursor.execute("""
        INSERT INTO patients (age, outcome)
        VALUES (%s, %s)
        """, (row['Age'], row['Outcome']))

    patient_id = cursor.lastrowid  # Get the last inserted patient_id

    # Inserting pregnancies, glucose, bp, skin thickness, insulin, BMI, and diabetes pedigree function into the health_metrics table
    cursor.execute("""
        INSERT INTO health_metrics (patient_id, pregnancies, glucose, blood_pressure, skin_thickness, insulin, bmi, diab_pedigree_func)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (patient_id, row['Pregnancies'], row['Glucose'], row['BloodPressure'], row['SkinThickness'], row['Insulin'], row['BMI'], row['DiabetesPedigreeFunction']))

conn.commit()
cursor.close()
conn.close()

# Debugging: Check column names
print(df.columns)

import mysql.connector

conn = mysql.connector.connect(
    host="localhost",
    user="user1",
    passwd="njusalisa",
    database="healthcare_db"
)
cursor = conn.cursor()

# Print first 5 entries from Patients Table
cursor.execute("SELECT * FROM patients LIMIT 5")
patients = cursor.fetchall()
print("First 5 entries in Patients Table:")
for patient in patients:
    print(patient)

# Print first 5 entries from Health Metrics Table
cursor.execute("SELECT * FROM health_metrics LIMIT 5")
health_metrics = cursor.fetchall()
print("\nFirst 5 entries in Health Metrics Table:")
for metric in health_metrics:
    print(metric)

# Close connection
cursor.close()
conn.close()

"""**Step 3:** Data Cleaning"""

from sklearn.impute import SimpleImputer

# Load the data
file_path = '/mnt/data/Healthcare-Diabetes.csv'
df = pd.read_csv("/Healthcare-Diabetes.csv")
print("Original DataFrame:")
print(df.head(20))

# Step 1: Select the first 20 rows
df_20 = df.head(20)
print("\nFirst 20 rows of the DataFrame:")
print(df_20)

# Step 2: Remove duplicate rows
df_20 = df_20.drop_duplicates()
print("\nDataFrame after removing duplicates:")
print(df_20)

# Step 3: Resolve inconsistencies
# **Identify actual categorical columns and apply appropriate transformations if needed.**
# For this example, we'll assume 'Outcome' is a categorical column
if 'Outcome' in df_20.columns:
    df_20['Outcome'] = df_20['Outcome'].astype(str)  # Convert to string type

print("\nDataFrame after resolving inconsistencies:")
print(df_20)

# Step 4: Handle missing values
# Separate numerical and categorical columns
numerical_cols = df_20.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = df_20.select_dtypes(include=['object']).columns

# Handle missing values for numerical columns
imputer_num = SimpleImputer(strategy='mean')
df_20[numerical_cols] = imputer_num.fit_transform(df_20[numerical_cols])

# Handle missing values for categorical columns **only if there are categorical columns**
if categorical_cols.size > 0:
    imputer_cat = SimpleImputer(strategy='most_frequent')
    df_20[categorical_cols] = imputer_cat.fit_transform(df_20[categorical_cols])

print("\nDataFrame after handling missing values:")
print(df_20)

"""**Step 4:** Data transformation"""

# Install necessary libraries
!pip install pandas scikit-learn

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the data
file_path = '/mnt/data/Healthcare-Diabetes.csv'
df = pd.read_csv("/Healthcare-Diabetes.csv")
print("Original DataFrame:")
print(df.head(20))

# Step 1: Select the first 20 rows
df_20 = df.head(20)
print("\nFirst 20 rows of the DataFrame:")
print(df_20)

# Feature Engineering

# 1. Age Category
def categorize_age(age):
    if age < 30:
        return 'Young'
    elif 30 <= age < 60:
        return 'Middle-aged'
    else:
        return 'Senior'

if 'Age' in df_20.columns:
    df_20['Age Category'] = df_20['Age'].apply(categorize_age)

# 2. Insulin Level
def categorize_insulin(insulin):
    if insulin < 50:
        return 'Low'
    elif 50 <= insulin < 200:
        return 'Normal'
    else:
        return 'High'

if 'Insulin' in df_20.columns:
    df_20['Insulin Level'] = df_20['Insulin'].apply(categorize_insulin)

# 3. Blood Pressure Status
def categorize_bp(bp):
    if bp < 80:
        return 'Low'
    elif 80 <= bp < 120:
        return 'Normal'
    else:
        return 'High'

if 'BloodPressure' in df_20.columns:
    df_20['Blood Pressure Status'] = df_20['BloodPressure'].apply(categorize_bp)

print("\nDataFrame after creating new features:")
print(df_20)

# Encoding Categorical Features
categorical_cols = df_20.select_dtypes(include=['object']).columns

# Apply OneHotEncoder to categorical columns
onehotencoder = OneHotEncoder()

# Transform the categorical columns
df_20_encoded = pd.DataFrame(onehotencoder.fit_transform(df_20[categorical_cols]).toarray(), columns=onehotencoder.get_feature_names_out(categorical_cols))

# Concatenate the encoded columns with the original dataframe (dropping the original categorical columns)
df_20 = df_20.drop(categorical_cols, axis=1)
df_20 = pd.concat([df_20, df_20_encoded], axis=1)

print("\nDataFrame after encoding categorical features:")
print(df_20)

# Normalize/Scale Numerical Data
numerical_cols = df_20.select_dtypes(include=['float64', 'int64']).columns

# Apply StandardScaler to numerical columns
scaler = StandardScaler()
df_20[numerical_cols] = scaler.fit_transform(df_20[numerical_cols])

print("\nDataFrame after normalizing/scaling numerical features:")
print(df_20)

#logistic regression

# Install necessary libraries
!pip install pandas scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Load the data
file_path = '/mnt/data/Healthcare-Diabetes.csv'
df = pd.read_csv("/Healthcare-Diabetes.csv")

# Select the first 20 rows for demonstration purposes
df = df.head(20)

# Preprocess the data
# Separate features and target variable
X = df.drop('Outcome', axis=1)  # Assuming 'Outcome' is the target variable
y = df['Outcome']

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns

# Preprocessing pipeline for numerical data
numerical_transformer = StandardScaler()

# Preprocessing pipeline for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Create and train the logistic regression model
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', LogisticRegression())])

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

#neural networks

# Install necessary libraries
!pip install pandas scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Load the data
file_path = '/mnt/data/Healthcare-Diabetes.csv'
df = pd.read_csv("/Healthcare-Diabetes.csv")

# Select the first 20 rows for demonstration purposes
df = df.head(20)

# Preprocess the data
# Separate features and target variable
X = df.drop('Outcome', axis=1)  # Assuming 'Outcome' is the target variable
y = df['Outcome']

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns

# Preprocessing pipeline for numerical data
numerical_transformer = StandardScaler()

# Preprocessing pipeline for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Create and train the neural network model
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MLPClassifier(max_iter=1000, random_state=0))])

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

#decision trees

# Install necessary libraries
!pip install pandas scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Load the data
file_path = '/mnt/data/Healthcare-Diabetes.csv'
df = pd.read_csv("/Healthcare-Diabetes.csv")


# Select the first 20 rows for demonstration purposes
df = df.head(20)

# Preprocess the data
# Separate features and target variable
X = df.drop('Outcome', axis=1)  # Assuming 'Outcome' is the target variable
y = df['Outcome']

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns

# Preprocessing pipeline for numerical data
numerical_transformer = StandardScaler()

# Preprocessing pipeline for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Create and train the decision tree model
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', DecisionTreeClassifier(random_state=0))])

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""**Step 5:** Data Visualization & Exploratory Data Analysis"""

# Here we are creating subplots for each type of medical data (Age, Pregnancies, Glucose, etc)
# This is good for visualizing distributions of data
fig, ax = plt.subplots(4, 2, figsize=(20, 20))

sns.histplot(df.Pregnancies, bins=20, ax=ax[0,0], color="red", kde=True)
sns.histplot(df.Glucose, bins=20, ax=ax[0,1], color="orange", kde=True)
sns.histplot(df.BloodPressure, bins=20, ax=ax[1,0], color="brown", kde=True)
sns.histplot(df.SkinThickness, bins=20, ax=ax[1,1], color="green", kde=True)
sns.histplot(df.Insulin, bins=20, ax=ax[2,0], color="blue", kde=True)
sns.histplot(df.BMI, bins=20, ax=ax[2,1], color="indigo", kde=True)
sns.histplot(df.DiabetesPedigreeFunction, bins=20, ax=ax[3,0], color="violet", kde=True)
sns.histplot(df.Age, bins=20, ax=ax[3,1], color="pink", kde=True)

plt.tight_layout()
plt.show()

# Drop the id column, since it isn't needed for correlation map
df = df.drop(columns=['Id'])

corr = df.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Medical Correlation Heatmap')
plt.show()

# pair plot
p = sns.pairplot(df, hue="Outcome")

from sklearn.ensemble import RandomForestClassifier
import numpy as np

# X_train, y_train from your data preprocessing steps above
model = RandomForestClassifier()
model.fit(X_train, y_train)

importances = model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 6))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# y_test and y_pred from your model predictions earlier
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()